#name: Qiuyu Fan
#date: 6.11.2018
#This is the code for regression and model validation

#2
learning2014<- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
str(learning2014)
dim(learning2014)

#the dataset learn2014 is a data frame and has 183 rows and 60 columns.

#3
# gender, age, attitude, deep, stra, surf and points 



learning2014$attitude <- learning2014$Attitude / 10
library(dplyr)
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
learning2014$deep <- rowMeans (select(learning2014, one_of(deep_questions)))
learning2014$surf <- rowMeans(select(learning2014, one_of(surface_questions)))
learning2014$stra <- rowMeans(select(learning2014, one_of(strategic_questions)))
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
newlearning2014 <- select(learning2014, one_of(keep_columns))
str(newlearning2014)
colnames(newlearning2014)[2] <- "age"
colnames(newlearning2014)[7] <- "points"
newlearning2014 <- filter(newlearning2014,points > 0)
str(newlearning2014)

#4

getwd()

setwd("/Users/qiuyufan/IODS-project")
write.csv(newlearning2014, file = "newlearning2014.csv",row.names=FALSE)
openlearning2014 <- read.csv(file="newlearning2014.csv", header=TRUE, sep=",")
str(openlearning2014)



#Analysis

#1 
openlearning2014 <- read.csv(file="newlearning2014.csv", header=TRUE, sep=",")
str(openlearning2014)
#This dataset contains 7 columns and 166 rows. attitude is the average of the Likert scale; points are the exam points; stra represents the average score of the strategic questions
#surf means the average score of the surface questions. Deep is the average score of the deep questions.

#2

library(ggplot2)
p1 <- ggplot(newlearning2014, aes(x = attitude, y = points, col = gender))
p2 <- p1 + geom_point()
p3 <- p2 + geom_smooth(method = "lm")
p4 <- p3 + ggtitle("Student's attitude versus exam points")
p4
#As the p4 shows, student's attitude is positively coorelated with exam points in both genders. 

#3

pairs(newlearning2014[-1], col = newlearning2014$gender)
install.packages("GGally")
library(GGally)
library(ggplot2)
p <- ggpairs(newlearning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
#based on the plot, strongest coorelation with points are: attitude, stra, and surf. 
#we then choose these three variables as parameters
ggpairs(newlearning2014, lower = list(combo = wrap("facethist", bins = 20)))
my_model2 <- lm(points ~ attitude + surf + stra, data = newlearning2014)
summary(my_model2)
#The three parameters, attitude, structure and surface questions are significantly importance 
#to the final scores. But only attitude is statistically significant with the points. 

#4 after removing the parameters of stra and surf. we have our model3, attitude is the only variable hitting 
#significance level, below 0.05
my_model3 <- lm(points ~ attitude, data = newlearning2014)
summary(my_model3)
#After different combinations with the variables, I decide to keep this model for comparison, 
#although the stra has p-value of 0.089
my_model4 <- lm(points ~ attitude + stra, data = newlearning2014)
summary(my_model4)
#the multiple R-squared in model3 is 0.1906, which indicate that this model explains 19% of all the variablity 
#of the data around the mean
#model4 is 0.2074, which indicate this model explain 20% of all the variablility of the data around the mean
#Normally, higher R-squared, the better the model fits the data. But in psychology, or tests
#involved in human behaviors, this value could be lower than 50%. 

#5
par(mfrow = c(2,2))
plot(my_model3, which = c(1,2,5))

par(mfrow = c(2,2))
plot(my_model4, which = c(1,2,5))

#Both models' QQ plot from the show that the errors of the models are normally distributed, which are reasonable.
#The residuals vs fitted plot are reasonably distributed and no patter in the scatter plot is found
#residuals vs leverage plot shows no standingout outliner. So I am confident to conclude they are good models. 
#if I must choose between them, I could choose model3, since it is the simplest and not that much 
#difference to model4, which have 2 variables. 


